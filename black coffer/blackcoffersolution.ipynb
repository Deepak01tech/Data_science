{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613cc028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article with URL_ID 123.0 to 123.0.txt\n",
      "Saved article with URL_ID 321.0 to 321.0.txt\n",
      "Saved article with URL_ID 2345.0 to 2345.0.txt\n",
      "Saved article with URL_ID 4321.0 to 4321.0.txt\n",
      "Saved article with URL_ID 432.0 to 432.0.txt\n",
      "Saved article with URL_ID 2893.8 to 2893.8.txt\n",
      "Saved article with URL_ID 3355.6 to 3355.6.txt\n",
      "Saved article with URL_ID 3817.4 to 3817.4.txt\n",
      "Saved article with URL_ID 4279.2 to 4279.2.txt\n",
      "Saved article with URL_ID 4741.0 to 4741.0.txt\n",
      "Saved article with URL_ID 5202.8 to 5202.8.txt\n",
      "Saved article with URL_ID 5664.6 to 5664.6.txt\n",
      "Saved article with URL_ID 6126.4 to 6126.4.txt\n",
      "Saved article with URL_ID 6588.2 to 6588.2.txt\n",
      "Saved article with URL_ID 7050.0 to 7050.0.txt\n",
      "Saved article with URL_ID 7511.8 to 7511.8.txt\n",
      "Saved article with URL_ID 7973.6 to 7973.6.txt\n",
      "Saved article with URL_ID 8435.4 to 8435.4.txt\n",
      "Saved article with URL_ID 8897.2 to 8897.2.txt\n",
      "Saved article with URL_ID 9359.0 to 9359.0.txt\n",
      "Saved article with URL_ID 9820.8 to 9820.8.txt\n",
      "Saved article with URL_ID 10282.6 to 10282.6.txt\n",
      "Saved article with URL_ID 10744.4 to 10744.4.txt\n",
      "Saved article with URL_ID 11206.2 to 11206.2.txt\n",
      "Error processing URL_ID 11668.0: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Saved article with URL_ID 12129.8 to 12129.8.txt\n",
      "Saved article with URL_ID 12591.6 to 12591.6.txt\n",
      "Saved article with URL_ID 13053.4 to 13053.4.txt\n",
      "Saved article with URL_ID 13515.2 to 13515.2.txt\n",
      "Saved article with URL_ID 13977.0 to 13977.0.txt\n",
      "Saved article with URL_ID 14438.8 to 14438.8.txt\n",
      "Saved article with URL_ID 14900.6 to 14900.6.txt\n",
      "Saved article with URL_ID 15362.4 to 15362.4.txt\n",
      "Saved article with URL_ID 15824.2 to 15824.2.txt\n",
      "Saved article with URL_ID 16286.0 to 16286.0.txt\n",
      "Saved article with URL_ID 16747.8 to 16747.8.txt\n",
      "Saved article with URL_ID 17209.6 to 17209.6.txt\n",
      "Error processing URL_ID 17671.4: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Saved article with URL_ID 18133.2 to 18133.2.txt\n",
      "Saved article with URL_ID 18595.0 to 18595.0.txt\n",
      "Saved article with URL_ID 19056.8 to 19056.8.txt\n",
      "Saved article with URL_ID 19518.6 to 19518.6.txt\n",
      "Saved article with URL_ID 19980.4 to 19980.4.txt\n",
      "Saved article with URL_ID 20442.2 to 20442.2.txt\n",
      "Saved article with URL_ID 20904.0 to 20904.0.txt\n",
      "Saved article with URL_ID 21365.8 to 21365.8.txt\n",
      "Saved article with URL_ID 21827.6 to 21827.6.txt\n",
      "Saved article with URL_ID 22289.4 to 22289.4.txt\n",
      "Saved article with URL_ID 22751.2 to 22751.2.txt\n",
      "Saved article with URL_ID 23213.0 to 23213.0.txt\n",
      "Saved article with URL_ID 23674.8 to 23674.8.txt\n",
      "Saved article with URL_ID 24136.6 to 24136.6.txt\n",
      "Saved article with URL_ID 24598.4 to 24598.4.txt\n",
      "Saved article with URL_ID 25060.2 to 25060.2.txt\n",
      "Saved article with URL_ID 25522.0 to 25522.0.txt\n",
      "Saved article with URL_ID 25983.8 to 25983.8.txt\n",
      "Saved article with URL_ID 26445.6 to 26445.6.txt\n",
      "Saved article with URL_ID 26907.4 to 26907.4.txt\n",
      "Saved article with URL_ID 27369.2 to 27369.2.txt\n",
      "Saved article with URL_ID 27831.0 to 27831.0.txt\n",
      "Saved article with URL_ID 28292.8 to 28292.8.txt\n",
      "Saved article with URL_ID 28754.6 to 28754.6.txt\n",
      "Saved article with URL_ID 29216.4 to 29216.4.txt\n",
      "Saved article with URL_ID 29678.2 to 29678.2.txt\n",
      "Saved article with URL_ID 30140.0 to 30140.0.txt\n",
      "Saved article with URL_ID 30601.8 to 30601.8.txt\n",
      "Saved article with URL_ID 31063.6 to 31063.6.txt\n",
      "Saved article with URL_ID 31525.4 to 31525.4.txt\n",
      "Saved article with URL_ID 31987.2 to 31987.2.txt\n",
      "Saved article with URL_ID 32449.0 to 32449.0.txt\n",
      "Saved article with URL_ID 32910.8 to 32910.8.txt\n",
      "Saved article with URL_ID 33372.6 to 33372.6.txt\n",
      "Saved article with URL_ID 33834.4 to 33834.4.txt\n",
      "Saved article with URL_ID 34296.2 to 34296.2.txt\n",
      "Saved article with URL_ID 34758.0 to 34758.0.txt\n",
      "Saved article with URL_ID 35219.8 to 35219.8.txt\n",
      "Saved article with URL_ID 35681.6 to 35681.6.txt\n",
      "Saved article with URL_ID 36143.4 to 36143.4.txt\n",
      "Saved article with URL_ID 36605.2 to 36605.2.txt\n",
      "Saved article with URL_ID 37067.0 to 37067.0.txt\n",
      "Saved article with URL_ID 37528.8 to 37528.8.txt\n",
      "Saved article with URL_ID 37990.6 to 37990.6.txt\n",
      "Saved article with URL_ID 38452.4 to 38452.4.txt\n",
      "Saved article with URL_ID 38914.2 to 38914.2.txt\n",
      "Saved article with URL_ID 39376.0 to 39376.0.txt\n",
      "Saved article with URL_ID 39837.8 to 39837.8.txt\n",
      "Saved article with URL_ID 40299.6 to 40299.6.txt\n",
      "Saved article with URL_ID 40761.4 to 40761.4.txt\n",
      "Saved article with URL_ID 41223.2 to 41223.2.txt\n",
      "Saved article with URL_ID 41685.0 to 41685.0.txt\n",
      "Saved article with URL_ID 42146.8 to 42146.8.txt\n",
      "Saved article with URL_ID 42608.6 to 42608.6.txt\n",
      "Saved article with URL_ID 43070.4 to 43070.4.txt\n",
      "Saved article with URL_ID 43532.2 to 43532.2.txt\n",
      "Saved article with URL_ID 43994.0 to 43994.0.txt\n",
      "Saved article with URL_ID 44455.8 to 44455.8.txt\n",
      "Saved article with URL_ID 44917.6 to 44917.6.txt\n",
      "Saved article with URL_ID 45379.4 to 45379.4.txt\n",
      "Saved article with URL_ID 45841.2 to 45841.2.txt\n",
      "Saved article with URL_ID 46303.0 to 46303.0.txt\n",
      "Saved article with URL_ID 46764.8 to 46764.8.txt\n",
      "Saved article with URL_ID 47226.6 to 47226.6.txt\n",
      "Saved article with URL_ID 47688.4 to 47688.4.txt\n",
      "Saved article with URL_ID 48150.2 to 48150.2.txt\n",
      "Saved article with URL_ID 48612.0 to 48612.0.txt\n",
      "Saved article with URL_ID 49073.8 to 49073.8.txt\n",
      "Saved article with URL_ID 49535.6 to 49535.6.txt\n",
      "Saved article with URL_ID 49997.4 to 49997.4.txt\n",
      "Saved article with URL_ID 50459.2 to 50459.2.txt\n",
      "Saved article with URL_ID 50921.0 to 50921.0.txt\n",
      "Saved article with URL_ID 51382.8 to 51382.8.txt\n",
      "Saved article with URL_ID 51844.6 to 51844.6.txt\n",
      "Saved article with URL_ID 52306.4 to 52306.4.txt\n",
      "Saved article with URL_ID 52768.2 to 52768.2.txt\n",
      "Extraction and saving completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the input.xlsx file containing URLs\n",
    "df = pd.read_excel('input.xlsx')\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the article title element (modify this according to your specific HTML structure)\n",
    "        title_element = soup.find('h1', class_='article-title')\n",
    "        article_title = title_element.text if title_element else 'Title not found'\n",
    "\n",
    "        # Find the article text element (modify this according to your specific HTML structure)\n",
    "        text_element = soup.find('div', class_='article-text')\n",
    "        article_text = text_element.text if text_element else 'Text not found'\n",
    "\n",
    "        # Save the extracted article title and text to a text file with URL_ID as the filename\n",
    "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(article_title + '\\n\\n')\n",
    "            file.write(article_text)\n",
    "\n",
    "        print(f\"Saved article with URL_ID {url_id} to {url_id}.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL_ID {url_id}: {str(e)}\")\n",
    "\n",
    "print(\"Extraction and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e328e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dp559\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\dp559\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/636.8 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/636.8 kB 325.1 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 143.4/636.8 kB 944.1 kB/s eta 0:00:01\n",
      "     ------------------------ ------------- 409.6/636.8 kB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 636.8/636.8 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f4d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL_ID 11668.0: [Errno 2] No such file or directory: '11668.0.txt'\n",
      "Error processing URL_ID 17671.4: [Errno 2] No such file or directory: '17671.4.txt'\n",
      "Textual analysis and variable computation completed. Output saved to 'Output_Computed_Variables.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the output structure Excel file\n",
    "output_structure = pd.read_excel(\"Output Data Structure.xlsx\")\n",
    "\n",
    "# Create lists to store the computed variables\n",
    "url_ids = []\n",
    "sentiment_polarity = []\n",
    "sentiment_subjectivity = []\n",
    "\n",
    "# Iterate through the rows of the DataFrame containing extracted articles\n",
    "for index, row in output_structure.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    try:\n",
    "        # Load the corresponding extracted article text\n",
    "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            article_text = file.read()\n",
    "\n",
    "        # Perform textual analysis using TextBlob\n",
    "        analysis = TextBlob(article_text)\n",
    "\n",
    "        # Compute sentiment polarity and subjectivity\n",
    "        polarity = analysis.sentiment.polarity\n",
    "        subjectivity = analysis.sentiment.subjectivity\n",
    "\n",
    "        # Append the computed values to the lists\n",
    "        url_ids.append(url_id)\n",
    "        sentiment_polarity.append(polarity)\n",
    "        sentiment_subjectivity.append(subjectivity)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors for missing or invalid text files\n",
    "        print(f\"Error processing URL_ID {url_id}: {str(e)}\")\n",
    "\n",
    "# Create a new DataFrame to store the computed variables\n",
    "output_data = pd.DataFrame({\n",
    "    'URL_ID': url_ids,\n",
    "    'Sentiment_Polarity': sentiment_polarity,\n",
    "    'Sentiment_Subjectivity': sentiment_subjectivity\n",
    "})\n",
    "\n",
    "# Save the output DataFrame to a new Excel file\n",
    "output_data.to_excel('Output_Computed_Variables.xlsx', index=False)\n",
    "\n",
    "print(\"Textual analysis and variable computation completed. Output saved to 'Output_Computed_Variables.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00778c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dp559\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dp559\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae0e001",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dp559/nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dp559/nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download stopwords if not already downloaded\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#nltk.download('stopwords')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load stopwords\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize and clean the text\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dp559/nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dp559\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and clean the text\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c32fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
